{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c16aa555",
   "metadata": {},
   "source": [
    "# Phi-4-Multimodalで音声翻訳\n",
    "\n",
    "Copied and modified from https://github.com/huggingface/open_asr_leaderboard/blob/main/phi/run_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552e2e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only required for Google Colab\n",
    "# ページ上部バーにて，ランタイム -> ランタイムのタイプを変更へと進み，ハードウェア アクセラレータとして\"T4 GPU\"を選択して保存\n",
    "!git clone https://github.com/ryota-komatsu/slp2025\n",
    "%cd slp2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ab0af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed982ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -t 0 -c -P data/LibriSpeech https://www.openslr.org/resources/12/test-clean.tar.gz\n",
    "!tar zxvf data/LibriSpeech/test-clean.tar.gz -C data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414f78bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from IPython.display import Audio\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, StoppingCriteria, StoppingCriteriaList\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24410df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleTokenBatchStoppingCriteria(StoppingCriteria):\n",
    "    \"\"\"Stopping criteria capable of receiving multiple stop-tokens and handling batched inputs.\"\"\"\n",
    "\n",
    "    def __init__(self, stop_tokens: torch.LongTensor, batch_size: int = 1) -> None:\n",
    "        \"\"\"Initialize the multiple token batch stopping criteria.\n",
    "\n",
    "        Args:\n",
    "            stop_tokens: Stop-tokens.\n",
    "            batch_size: Batch size.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.stop_tokens = stop_tokens\n",
    "        self.max_stop_tokens = stop_tokens.shape[-1]\n",
    "        self.stop_tokens_idx = torch.zeros(batch_size, dtype=torch.long, device=stop_tokens.device)\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        # Only gather the maximum number of inputs compatible with stop tokens\n",
    "        # and checks whether generated inputs are equal to `stop_tokens`\n",
    "        generated_inputs = torch.eq(input_ids[:, -self.max_stop_tokens :].unsqueeze(1), self.stop_tokens)\n",
    "        equal_generated_inputs = torch.all(generated_inputs, dim=2)\n",
    "\n",
    "        # Mark the position where a stop token has been produced for each input in the batch,\n",
    "        # but only if the corresponding entry is not already set\n",
    "        sequence_idx = torch.any(equal_generated_inputs, dim=1)\n",
    "        sequence_set_mask = self.stop_tokens_idx == 0\n",
    "        self.stop_tokens_idx[sequence_idx & sequence_set_mask] = input_ids.shape[-1]\n",
    "\n",
    "        return torch.all(self.stop_tokens_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aab3fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_or_path=\"microsoft/Phi-4-multimodal-instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    name_or_path,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=\"auto\",\n",
    "    _attn_implementation=\"flash_attention_2\" if is_flash_attn_2_available() else \"sdpa\",\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(name_or_path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff55f769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template\n",
    "user = \"<|user|>\"\n",
    "assistant = \"<|assistant|>\"\n",
    "prompt_suffix = \"<|end|>\"\n",
    "lang = \"Japanese\"\n",
    "prompt = f\"{user}<|audio_1|>Transcribe the audio to text, and then translate the audio to {lang}. Use <sep> as a separator between the original transcript and the translation.{prompt_suffix}{assistant}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e6e27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for generation\n",
    "max_new_tokens = 512\n",
    "num_beams = 1\n",
    "num_logits_to_keep = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41578584",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"data/LibriSpeech/test-clean/61/70968/61-70968-0021.flac\"\n",
    "audio, sr = torchaudio.load(audio_path)\n",
    "audios = [(audio.numpy(), sr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3050586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_tokens = [prompt_suffix, processor.tokenizer.eos_token]\n",
    "stop_tokens_ids = processor.tokenizer(\n",
    "    stop_tokens, add_special_tokens=False, padding=\"longest\", return_tensors=\"pt\"\n",
    ")[\"input_ids\"]\n",
    "stop_tokens_ids = stop_tokens_ids.to(model.device)\n",
    "\n",
    "# Load audio inputs\n",
    "minibatch_size = len(audios)\n",
    "gen_kwargs = {\n",
    "    \"max_new_tokens\": max_new_tokens,\n",
    "    \"num_beams\": num_beams,\n",
    "    \"stopping_criteria\": StoppingCriteriaList(\n",
    "        [MultipleTokenBatchStoppingCriteria(stop_tokens_ids, batch_size=num_beams * minibatch_size)]\n",
    "    ),\n",
    "}\n",
    "\n",
    "with torch.autocast(model.device.type, enabled=True):\n",
    "    inputs = processor(text=[prompt] * minibatch_size, audios=audios, return_tensors=\"pt\").to(\n",
    "        model.device\n",
    "    )\n",
    "\n",
    "    # Model Inference\n",
    "    pred_ids = model.generate(\n",
    "        **inputs,\n",
    "        pad_token_id=processor.tokenizer.pad_token_id,\n",
    "        eos_token_id=processor.tokenizer.eos_token_id,\n",
    "        **gen_kwargs,\n",
    "        num_logits_to_keep=num_logits_to_keep,\n",
    "    )\n",
    "\n",
    "# Gather the sequence index of the stop token\n",
    "stop_tokens_idx = gen_kwargs[\"stopping_criteria\"][0].stop_tokens_idx.reshape(minibatch_size, -1)[:, 0]\n",
    "\n",
    "# If a stop token was produced, we need to remove its length from the found index,\n",
    "# however there might be a chance that the stop token was not produced and the index\n",
    "# returned is the length of the generated sequence\n",
    "stop_tokens_idx = torch.where(\n",
    "    stop_tokens_idx > 0,\n",
    "    stop_tokens_idx - stop_tokens_ids.shape[-1],\n",
    "    pred_ids.shape[-1],\n",
    ")\n",
    "\n",
    "# Convert token ids to text transcription\n",
    "pred_text = [\n",
    "    processor.decode(\n",
    "        _pred_ids[inputs[\"input_ids\"].shape[1] : _stop_tokens_idx],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False,\n",
    "    )\n",
    "    for _pred_ids, _stop_tokens_idx in zip(pred_ids, stop_tokens_idx)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6f1ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(audio, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fe09e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_text"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
