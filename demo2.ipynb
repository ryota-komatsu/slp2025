{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "911a71dd",
   "metadata": {},
   "source": [
    "# Llama 3.2とWhisper encoderをadapterで接続してzero-shot instruction following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e8380b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import Audio, load_dataset\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoProcessor,\n",
    "    PretrainedConfig,\n",
    "    PreTrainedModel,\n",
    "    WhisperForConditionalGeneration,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5114bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adapter(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_hidden_size: int,\n",
    "        decoder_hidden_size: int,\n",
    "        kernel_size: int,\n",
    "        bias: bool,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pool = nn.AvgPool1d(kernel_size)\n",
    "        self.linear1 = nn.Linear(encoder_hidden_size, 2 * decoder_hidden_size, bias=bias)\n",
    "        self.linear2 = nn.Linear(2 * decoder_hidden_size, decoder_hidden_size, bias=bias)\n",
    "\n",
    "    def forward(self, hidden_states: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        hidden_states = hidden_states.permute(0, 2, 1)\n",
    "        hidden_states = self.pool(hidden_states)\n",
    "        hidden_states = hidden_states.permute(0, 2, 1)\n",
    "        hidden_states = self.linear1(hidden_states)\n",
    "        hidden_states = F.gelu(hidden_states)\n",
    "        hidden_states = self.linear2(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1916c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaForSpeechLMConfig(PretrainedConfig):\n",
    "    model_type = \"llama_for_speech_lm\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_id: str = \"openai/whisper-small.en\",\n",
    "        decoder_id: str = \"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "        adapter_kernel_size: int = 4,\n",
    "        adapter_linear_bias: bool = False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.encoder_id = encoder_id\n",
    "        self.decoder_id = decoder_id\n",
    "        self.adapter_kernel_size = adapter_kernel_size\n",
    "        self.adapter_linear_bias = adapter_linear_bias\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "\n",
    "class LlamaForSpeechLM(PreTrainedModel):\n",
    "    config_class = LlamaForSpeechLMConfig\n",
    "    _tied_weights_keys = [\"decoder.lm_head.weight\"]\n",
    "\n",
    "    def __init__(self, config: LlamaForSpeechLMConfig):\n",
    "        super().__init__(config)\n",
    "        self.encoder = WhisperForConditionalGeneration.from_pretrained(config.encoder_id).model.encoder\n",
    "        self.decoder = AutoModelForCausalLM.from_pretrained(config.decoder_id, torch_dtype=torch.bfloat16)\n",
    "        self.adapter = Adapter(\n",
    "            self.encoder.config.d_model,\n",
    "            self.decoder.config.hidden_size,\n",
    "            config.adapter_kernel_size,\n",
    "            config.adapter_linear_bias,\n",
    "        )\n",
    "\n",
    "        self.encoder.requires_grad_(False)\n",
    "        self.decoder.requires_grad_(False)\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.decoder.model.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.decoder.model.embed_tokens = value\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.decoder.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.decoder.lm_head = new_embeddings\n",
    "\n",
    "    def embed(\n",
    "        self,\n",
    "        input_features: torch.FloatTensor,\n",
    "        input_ids: torch.LongTensor,\n",
    "        encoder_attention_mask: torch.LongTensor,\n",
    "        decoder_attention_mask: torch.LongTensor,\n",
    "    ):\n",
    "        encoder_outputs = self.encoder(input_features)\n",
    "        encoder_hidden_states = encoder_outputs[0]\n",
    "\n",
    "        lengths = self.encoder._get_feat_extract_output_lengths(encoder_attention_mask.sum(dim=1, keepdim=True))\n",
    "        lengths = lengths // self.config.adapter_kernel_size\n",
    "        max_len = lengths.max()\n",
    "\n",
    "        encoder_hidden_states = self.adapter(encoder_hidden_states)\n",
    "        encoder_hidden_states = encoder_hidden_states[:, :max_len]\n",
    "\n",
    "        inputs_embeds = self.decoder.model.embed_tokens(input_ids)\n",
    "        inputs_embeds = torch.cat((encoder_hidden_states, inputs_embeds), dim=1)\n",
    "\n",
    "        attention_mask = torch.cat(\n",
    "            (\n",
    "                (\n",
    "                    torch.arange(encoder_hidden_states.shape[1], device=decoder_attention_mask.device).unsqueeze(0)\n",
    "                    < lengths\n",
    "                ).long(),\n",
    "                decoder_attention_mask,\n",
    "            ),\n",
    "            dim=1,\n",
    "        )\n",
    "        return inputs_embeds, attention_mask\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_features: torch.FloatTensor,\n",
    "        input_ids: torch.LongTensor,\n",
    "        encoder_attention_mask: torch.LongTensor,\n",
    "        decoder_attention_mask: torch.LongTensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_features (`torch.FloatTensor` of shape `(batch_size, feature_size, feature_length)`):\n",
    "                Log mel spectrogram.\n",
    "            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "                Token ids.\n",
    "            encoder_attention_mask (`torch.LongTensor` of shape `(batch_size, feature_length)`):\n",
    "                1: non-mask\n",
    "                0: mask\n",
    "            decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
    "                1: non-mask\n",
    "                0: mask\n",
    "        \"\"\"\n",
    "        inputs_embeds, attention_mask = self.embed(\n",
    "            input_features, input_ids, encoder_attention_mask, decoder_attention_mask\n",
    "        )\n",
    "\n",
    "        labels = F.pad(input_ids, (inputs_embeds.shape[1] - input_ids.shape[1], 0), value=-100)\n",
    "\n",
    "        decoder_outputs = self.decoder(inputs_embeds=inputs_embeds, attention_mask=attention_mask, labels=labels)\n",
    "        return decoder_outputs.loss\n",
    "\n",
    "    @torch.amp.autocast(\"cuda\", dtype=torch.bfloat16)\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        input_features: torch.FloatTensor,\n",
    "        input_ids: torch.LongTensor,\n",
    "        encoder_attention_mask: torch.LongTensor,\n",
    "        decoder_attention_mask: torch.LongTensor,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        inputs_embeds, attention_mask = self.embed(\n",
    "            input_features, input_ids, encoder_attention_mask, decoder_attention_mask\n",
    "        )\n",
    "\n",
    "        generated_ids = self.decoder.generate(inputs_embeds=inputs_embeds, attention_mask=attention_mask, **kwargs)\n",
    "        return generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf414001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_schedule(\n",
    "    optimizer,\n",
    "    total_steps: int,\n",
    "    warmup_steps: int,\n",
    "    base_lr: float,\n",
    "    min_lr: float,\n",
    ") -> torch.optim.lr_scheduler.LambdaLR:\n",
    "    def lr_schedule(current_step: int) -> float:\n",
    "        if current_step < warmup_steps:\n",
    "            return (min_lr + (base_lr - min_lr) * current_step / warmup_steps) / base_lr\n",
    "        else:\n",
    "            progress = (current_step - warmup_steps) / (total_steps - warmup_steps)\n",
    "            return (min_lr + (base_lr - min_lr) * (1 - progress)) / base_lr\n",
    "\n",
    "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a43f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train(\n",
    "    model: LlamaForSpeechLM,\n",
    "    loader: torch.utils.data.DataLoader,\n",
    "    lr: float = 1e-3,\n",
    "    epoch: int = 1,\n",
    "    warmup_steps: int = 10,\n",
    "    init_grad_scale: float = 1e32,\n",
    "    clip_grad_norm: float = 1.0,\n",
    "    grad_accumulation: int = 128,\n",
    "    model_dir=\"models/Llama-for-SpeechLM\",\n",
    "):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    # learning rate scheduler\n",
    "    lr_scheduler = get_lr_schedule(\n",
    "        optimizer,\n",
    "        len(loader) // grad_accumulation * epoch,\n",
    "        warmup_steps,\n",
    "        lr,\n",
    "        lr * 0.1,\n",
    "    )\n",
    "\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", init_scale=init_grad_scale)\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    step = 0\n",
    "\n",
    "    for epoch in range(1, epoch + 1):\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, batch in enumerate(tqdm(loader, desc=f\"epoch {epoch}\")):\n",
    "            with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "                loss = model(**batch)\n",
    "                loss = loss / grad_accumulation\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (batch_idx + 1) % grad_accumulation == 0:\n",
    "                # gradient clipping\n",
    "                scaler.unscale_(optimizer)\n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_grad_norm)\n",
    "\n",
    "                # update\n",
    "                scaler.step(optimizer)\n",
    "                scale = scaler.get_scale()\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # update learning rate\n",
    "                lr = lr_scheduler.get_last_lr()[0]\n",
    "                lr_scheduler.step()\n",
    "\n",
    "                step += 1\n",
    "\n",
    "                # tensorboard log\n",
    "                writer.add_scalar(\"train/loss\", loss.item(), step)\n",
    "                writer.add_scalar(\"train/lr\", lr, step)\n",
    "                writer.add_scalar(\"train/scale\", scale, step)\n",
    "                writer.add_scalar(\"train/grad_norm\", grad_norm.item(), step)\n",
    "\n",
    "        Path(model_dir).parent.mkdir(parents=True, exist_ok=True)\n",
    "        model.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940c157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collate_fn(encoder_processor, decoder_processor):\n",
    "    prompt = \"\"\"\n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "    Below is an instruction that describes a task, paired with an audio input that provides further context. Transcribe the audio clip into English, and then write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction:\n",
    "    {}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "    ### Transcript:\n",
    "    {}\n",
    "\n",
    "    ### Response:\n",
    "    {}<|eot_id|>\n",
    "    \"\"\"\n",
    "\n",
    "    def collate_fn(batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch: List of the following example:\n",
    "                {\n",
    "                    \"instruction\": \"\",\n",
    "                    \"input\": \"\",\n",
    "                    \"output\": \"\",\n",
    "                    \"text\": \"\",\n",
    "                    \"audio\": {\"path\": None, \"array\": tensor([...]), \"sampling_rate\": tensor(16000)},\n",
    "                }\n",
    "        \"\"\"\n",
    "\n",
    "        encoder_inputs = encoder_processor(\n",
    "            [item[\"audio\"][\"array\"].numpy() for item in batch],\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "            sampling_rate=16000,\n",
    "            device=\"cuda\",\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        decoder_inputs = decoder_processor(\n",
    "            [prompt.format(item[\"instruction\"], item[\"input\"], item[\"output\"]) for item in batch],\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        return {\n",
    "            \"input_features\": encoder_inputs.input_features,\n",
    "            \"input_ids\": decoder_inputs.input_ids,\n",
    "            \"encoder_attention_mask\": encoder_inputs.attention_mask,\n",
    "            \"decoder_attention_mask\": decoder_inputs.attention_mask,\n",
    "        }\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e5ba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(\n",
    "    model_id=\"ryota-komatsu/Llama-for-SpeechLM-Instruct\",\n",
    "    dataset_id=\"ryota-komatsu/spoken-alpaca\",\n",
    "    model_dir=\"models/Llama-for-SpeechLM-Instruct\",\n",
    "    batch_size: int = 4,\n",
    "    lr: float = 1e-3,\n",
    "    epoch: int = 5,\n",
    "    warmup_steps: int = 10,\n",
    "    init_grad_scale: float = 1e32,\n",
    "    clip_grad_norm: float = 1.0,\n",
    "    grad_accumulation: int = 128,\n",
    "):\n",
    "    model = LlamaForSpeechLM.from_pretrained(model_id).cuda()\n",
    "\n",
    "    encoder_processor = AutoProcessor.from_pretrained(model.config.encoder_id)\n",
    "    decoder_processor = AutoProcessor.from_pretrained(model.config.decoder_id)\n",
    "    decoder_processor.pad_token = decoder_processor.pad_token or decoder_processor.eos_token\n",
    "\n",
    "    def filter_by_length(example):\n",
    "        return (\n",
    "            len(example[\"audio\"][\"array\"]) < 16000 * 30\n",
    "            and len(example[\"instruction\"]) < 102\n",
    "            and len(example[\"output\"]) < 838\n",
    "        )\n",
    "\n",
    "    dataset = load_dataset(dataset_id, split=\"train\")\n",
    "    dataset = dataset.cast_column(\"audio\", Audio())\n",
    "    dataset = dataset.with_format(\"torch\")\n",
    "    dataset = dataset.filter(filter_by_length)\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size, True, collate_fn=get_collate_fn(encoder_processor, decoder_processor)\n",
    "    )\n",
    "\n",
    "    _train(\n",
    "        model,\n",
    "        loader,\n",
    "        lr,\n",
    "        epoch,\n",
    "        warmup_steps,\n",
    "        init_grad_scale,\n",
    "        clip_grad_norm,\n",
    "        grad_accumulation,\n",
    "        model_dir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feff303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(\n",
    "    encoder_id=\"openai/whisper-small.en\",\n",
    "    decoder_id=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    dataset_id=\"ryota-komatsu/spoken-alpaca\",\n",
    "    model_dir=\"models/Llama-for-SpeechLM-Instruct\",\n",
    "    max_length: int = 1024,\n",
    "    do_sample: bool = False,\n",
    "    num_beams: int = 5,\n",
    "):\n",
    "    model = LlamaForSpeechLM.from_pretrained(model_dir).cuda()\n",
    "\n",
    "    encoder_processor = AutoProcessor.from_pretrained(encoder_id)\n",
    "    decoder_processor = AutoProcessor.from_pretrained(decoder_id)\n",
    "    decoder_processor.pad_token = decoder_processor.pad_token or decoder_processor.eos_token\n",
    "\n",
    "    prompt = \"\"\"\n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "    Below is an instruction that describes a task, paired with an audio input that provides further context. Transcribe the audio clip into English, and then write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction:\n",
    "    {}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def filter_by_length(example):\n",
    "        return len(example[\"audio\"][\"array\"]) < 16000 * 30 and (\n",
    "            not len(example[\"instruction\"]) < 102 or not len(example[\"output\"]) < 838\n",
    "        )\n",
    "\n",
    "    dataset = load_dataset(dataset_id, split=\"train\")\n",
    "    dataset = dataset.with_format(\"torch\")\n",
    "    dataset = dataset.filter(filter_by_length)\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(dataset, shuffle=True)\n",
    "\n",
    "    for item in loader:\n",
    "        encoder_inputs = encoder_processor(\n",
    "            item[\"audio\"][\"array\"].numpy(),\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "            sampling_rate=16000,\n",
    "            device=\"cuda\",\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        decoder_inputs = decoder_processor(\n",
    "            prompt.format(item[\"instruction\"][0]),\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        generated_ids = model.generate(\n",
    "            encoder_inputs.input_features,\n",
    "            decoder_inputs.input_ids,\n",
    "            encoder_attention_mask=encoder_inputs.attention_mask,\n",
    "            decoder_attention_mask=decoder_inputs.attention_mask,\n",
    "            max_length=max_length,\n",
    "            do_sample=do_sample,\n",
    "            num_beams=num_beams,\n",
    "        )\n",
    "        generated_txt = decoder_processor.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
